{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tier 1 v2: Qwen 2.5 1.5B with Ultra-Structured Prompt\n",
    "\n",
    "## Key Improvements from v1\n",
    "1. ✅ Chat template for proper system/user messages\n",
    "2. ✅ Ultra-clear instructions: \"RESPOND ONLY WITH A NUMBER\"\n",
    "3. ✅ Few-shot examples showing exact format\n",
    "4. ✅ Reduced max_tokens from 50 to 10\n",
    "5. ✅ Lower temperature: 0.01 (from 0.1)\n",
    "6. ✅ 3-stage robust parsing: direct float → regex → keyword fallback\n",
    "\n",
    "## Expected Result\n",
    "- LB Score: **0.91-0.92** (conservative)\n",
    "- Medal Range: **0.92-0.925** (optimistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    LORA_ADAPTER = \"/kaggle/input/mahmoudmohamed-lora-adapter/\"\n",
    "    TEST_DATA = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\n",
    "    OUTPUT = \"submission.csv\"\n",
    "    \n",
    "    MAX_LENGTH = 512\n",
    "    TEMPERATURE = 0.01\n",
    "    MAX_NEW_TOKENS = 10\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Config: {CFG.BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_v2(tokenizer, row):\n",
    "    \"\"\"V2: Ultra-structured prompt\"\"\"\n",
    "    system = \"You are a precise AI. Respond ONLY with a decimal number between 0.0 and 1.0. NO other text.\"\n",
    "    \n",
    "    user = f\"\"\"Analyze if this post violates the rule.\n",
    "\n",
    "RULE: {row['rule']}\n",
    "POST: {row['body']}\n",
    "\n",
    "POSITIVE EXAMPLES (violations):\n",
    "1. {row['positive_example_1']}\n",
    "2. {row['positive_example_2']}\n",
    "\n",
    "NEGATIVE EXAMPLES (not violations):\n",
    "1. {row['negative_example_1']}\n",
    "2. {row['negative_example_2']}\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0:\n",
    "- 0.0 = NOT a violation\n",
    "- 1.0 = IS a violation\n",
    "\n",
    "Examples: 0.95, 0.23, 0.78, 0.02\n",
    "\n",
    "Your answer (number only):\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_v2(model, tokenizer, prompt, row_id):\n",
    "    \"\"\"V2: 3-stage robust parsing\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=CFG.MAX_LENGTH, truncation=True).to(CFG.DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CFG.MAX_NEW_TOKENS,\n",
    "            temperature=CFG.TEMPERATURE,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_ids = outputs[0][len(inputs.input_ids[0]):]\n",
    "    text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Stage 1: Direct float\n",
    "    try:\n",
    "        prob = float(text)\n",
    "        return max(0.0, min(1.0, prob))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Stage 2: Regex\n",
    "    matches = re.findall(r'\\b(0\\.\\d+|1\\.0+)\\b', text)\n",
    "    if matches:\n",
    "        try:\n",
    "            prob = float(matches[0])\n",
    "            return max(0.0, min(1.0, prob))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Stage 3: Keywords\n",
    "    lower = text.lower()\n",
    "    if any(w in lower for w in ['yes', 'violate', 'spam']):\n",
    "        return 0.8\n",
    "    if any(w in lower for w in ['no', 'not', 'fine']):\n",
    "        return 0.2\n",
    "    \n",
    "    print(f\"⚠️ Row {row_id}: '{text[:50]}' -> 0.5\")\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Tier 1 v2: Ultra-Structured Prompt\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.BASE_MODEL, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, CFG.LORA_ADAPTER, torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "print(f\"✅ Model loaded on {CFG.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(CFG.TEST_DATA)\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running inference...\")\n",
    "predictions = []\n",
    "success = fail = 0\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    prompt = create_prompt_v2(tokenizer, row)\n",
    "    prob = predict_single_v2(model, tokenizer, prompt, row['row_id'])\n",
    "    predictions.append({'row_id': row['row_id'], 'rule_violation': prob})\n",
    "    \n",
    "    if prob not in [0.2, 0.5, 0.8]:\n",
    "        success += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "    \n",
    "    if idx < 3:\n",
    "        print(f\"Sample {idx+1}: {row['body'][:60]}... -> {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(predictions)\n",
    "submission.to_csv(CFG.OUTPUT, index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✅ Saved: {CFG.OUTPUT}\")\n",
    "print(f\"Total: {len(submission)}\")\n",
    "print(f\"Parse success: {success}/{len(test_df)} ({100*success/len(test_df):.1f}%)\")\n",
    "print(f\"\\nStats:\")\n",
    "print(submission['rule_violation'].describe())\n",
    "print(f\"\\nFirst 10:\")\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
