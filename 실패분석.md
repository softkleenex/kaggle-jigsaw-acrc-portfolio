# 실패 분석: Qwen LoRA 추론이 실패한 이유

> **가설 → 증거 → 결론의 체계적 디버깅 과정**

[← 메인 README로 돌아가기](README_KR.md)

---

## 🎬 요약 (바쁜 분들을 위한 3줄)

1. **문제:** Qwen 2.5 1.5B 모델에 공개 LoRA 어댑터 적용 → 모든 예측값 0.0
2. **근본 원인:** 4B용 어댑터를 1.5B 모델에 로드 (베이스 모델 불일치)
3. **교훈:** Config만 믿지 말고 데이터셋 이름, 학습 아티팩트, 소규모 테스트 교차 검증 필수

---

## 📅 실패 타임라인

### 2024년 10월 22일 오전 10:00 - Tier 1 v1 시도

**시도 내용:**
```python
베이스 모델: Qwen/Qwen2.5-1.5B-Instruct
어댑터: mahmoudmohamed/reddit-4b-think (공개)
프롬프트: 기본 instruction format

def create_prompt(row):
    prompt = f"""You are a content moderator.
Determine if the following post violates the given rule.

Rule: {row['rule']}
Post to evaluate: {row['body']}

Does this post violate the rule?
Answer with a probability between 0 and 1.

Probability:"""
    return prompt
```

**결과:**
```
전체 10개 예측
→ 모두 0.0 😱

파싱 성공률: ~50%
실패 사례: "I don't think this violates" (숫자 없음)
```

**소요 시간:** 2시간

**첫 반응:**
> "아, 프롬프트가 너무 애매해서 모델이 숫자를 제대로 안 뽑아내는구나. 프롬프트를 더 명확하게 만들어야겠다."

---

### 2024년 10월 22일 오후 14:00 - Tier 1 v2 시도

**개선 사항:**

| 항목 | v1 | v2 (개선) |
|------|----|-----------|
| **Format** | Plain text | Chat template (system/user 분리) |
| **Instructions** | 애매함 | "ONLY NUMBER" 명시 |
| **Examples** | 없음 | Few-shot (0.95, 0.23, 0.78...) |
| **Temperature** | 0.1 | 0.01 (deterministic) |
| **Max tokens** | 50 | 10 (간결한 출력 강제) |
| **Parsing** | 단순 float() | 3단계 robust strategy |

**핵심 코드 (v2):**
```python
def create_prompt_v2(tokenizer, row):
    system = "You are a precise AI. Respond ONLY with a decimal number between 0.0 and 1.0. NO other text."

    user = f"""Analyze if this post violates the rule.

RULE: {row['rule']}
POST: {row['body']}

Respond with ONLY a number between 0.0 and 1.0:
- 0.0 = NOT a violation
- 1.0 = IS a violation

Examples: 0.95, 0.23, 0.78, 0.02

Your answer (number only):"""

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user}
    ]

    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

def predict_single_v2(model, tokenizer, prompt, row_id):
    # ... 생성 코드 ...

    # Stage 1: 직접 float 파싱
    try:
        prob = float(text)
        return max(0.0, min(1.0, prob))
    except:
        pass

    # Stage 2: Regex 추출
    matches = re.findall(r'\b(0\.\d+|1\.0+)\b', text)
    if matches:
        return max(0.0, min(1.0, float(matches[0])))

    # Stage 3: 키워드 기반 fallback
    if any(w in text.lower() for w in ['yes', 'violate']):
        return 0.8
    return 0.5
```

**결과:**
```
전체 10개 예측
→ 여전히 모두 0.0 😱😱

하지만...
파싱 성공률: 100% ✅

모든 출력:
Row 2029: "0.0" -> Parsed: 0.0 (Stage 1)
Row 2030: "0.0" -> Parsed: 0.0 (Stage 1)
Row 2031: "0.0" -> Parsed: 0.0 (Stage 1)
... (전부 동일)
```

**소요 시간:** 2시간

### 🚨 핵심 발견

```
파싱은 완벽한데 (100% 성공)
값이 전부 똑같다 (모두 0.0)

→ 이건 프롬프트 문제가 아니다!
```

모델이:
- ✅ 프롬프트 이해함 (chat template 정상 작동)
- ✅ 출력 형식 지킴 ("0.0" 정확히 출력)
- ❌ **하지만 추론은 안 함** (입력과 무관하게 0.0만)

**여기서 pivot:** 프롬프트가 아니라 **모델/어댑터 호환성 문제**일 가능성 높음

---

## 🔬 가설 수립 및 증거 수집

### 가설 A: 베이스 모델 불일치 (신뢰도 80%)

**주장:**
어댑터가 Qwen 4B용으로 학습되었는데, Qwen 1.5B에 로드함

**왜 이게 문제인가?**
```python
# LoRA의 동작 원리
# 기존 weight W에 low-rank update ΔW = AB를 더함
# W': (hidden_size, hidden_size)
# A:  (hidden_size, r)
# B:  (r, hidden_size)

# 만약 hidden_size가 다르면?
Qwen 1.5B hidden_size = 1536
Qwen 4B hidden_size   = 2560 (추정)

# Dimension mismatch!
# PEFT 라이브러리가 오류 없이 로드하지만 (graceful handling)
# weights가 제대로 align 안 됨 → degenerate output
```

**수집한 증거:**

#### 증거 1-1: Adapter Config

```json
// configs/mahmoudmohamed_adapter_config.json
{
  "base_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
  "lora_alpha": 32,
  "r": 16,
  "target_modules": ["q_proj", "v_proj"],
  "task_type": "CAUSAL_LM"
}
```

Config는 "1.5B용"이라고 주장 ✅

#### 증거 1-2: 데이터셋 이름 분석

```
데이터셋 slug: mahmoudmohamed/reddit-4b-think
                                    ^^^
                                    주목!
```

**"4b"가 눈에 띔!**

**해석:**
- Naming convention: `{author}/{task}-{model_size}-{method}`
- 예: `reddit-4b-think` = Reddit 데이터, **4B 모델**, thinking/reasoning 방식
- Config는 공유를 위해 편집했을 가능성
- 실제 학습은 Qwen 4B로 진행했을 가능성 **높음**

#### 증거 1-3: 학습 아티팩트 부재

```bash
# mahmoudmohamed 어댑터 파일 구조
mahmoudmohamed/
├── adapter_model.bin        ✅ (30MB)
├── adapter_config.json      ✅
├── README.md                ✅ (간단한 설명만)
└── train.pkl                ❌ 없음!
└── val.pkl                  ❌ 없음!
└── training_log.txt         ❌ 없음!
```

**문제점:**
- 실제로 학습했는지 검증 불가
- 어떤 데이터로 학습했는지 불명
- Validation split 없음 → 신뢰성 낮음

#### 증거 1-4: seojinpark 어댑터와 비교

```json
// configs/seojinpark_fold3_adapter_config.json
{
  "base_model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4",
  //                                               ^^^^^^^^^^^^^^
  //                               명시적으로 quantization variant까지!
  "lora_alpha": 16,
  "r": 8,
  "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"]
}
```

```bash
# seojinpark 어댑터 파일 구조
seojinpark/fold3/
├── adapter_model.bin        ✅ (15MB, r=8이라 더 작음)
├── adapter_config.json      ✅
├── train.pkl                ✅ 5.9MB
├── val.pkl                  ✅ 726KB
└── trained_model/           ✅ full checkpoint
```

**비교 결과:**

| 항목 | mahmoudmohamed | seojinpark | Winner |
|------|----------------|------------|--------|
| **Base model 명시성** | "1.5B-Instruct" (애매함) | "1.5B-Instruct-**GPTQ-Int4**" (명확) | ✅ seojinpark |
| **데이터셋 이름 일관성** | 🚩 "4b-think" (충돌!) | ✅ "jigsaw-qwen-fold3" | ✅ seojinpark |
| **학습 아티팩트** | ❌ 없음 | ✅ train.pkl, val.pkl | ✅ seojinpark |
| **K-fold 전략** | ❌ 단일 | ✅ fold0~4 전체 공개 | ✅ seojinpark |
| **내가 테스트한 결과** | ❌ 전부 0.0 | ⏱️ 미테스트 (대회 종료) | - |

**결론:**
seojinpark이 **제대로 된 공개 어댑터**의 모습입니다.

---

### 가설 B: 이진 분류 학습 (신뢰도 60%)

**주장:**
어댑터가 binary classification (0 or 1)으로 학습되었는데, 나는 continuous probability (0.0~1.0)를 요구함

**근거:**
- 모든 출력이 0.0 (경계값)
- 1.0이나 0.5가 아닌 0.0만 출력
- "안전한" 값으로 degenerate

**Counter-evidence:**
- 만약 binary라면 0.0과 1.0 둘 다 나와야 함
- 하지만 **오직 0.0만** 나옴
- 이건 binary 문제가 아니라 더 근본적인 문제

**결론:** 가능성은 있지만 가설 A보다 낮음

---

### 가설 C: 프롬프트 형식 불일치 (신뢰도 40%)

**주장:**
어댑터 학습 시 사용한 프롬프트 형식과 내 프롬프트 형식이 다름

**Counter-evidence (반증):**
- v2에서 100% 파싱 성공
- 모델이 chat template 이해하고 정확히 "0.0" 출력
- 만약 프롬프트 형식 문제면 파싱도 실패했을 것

**결론:** 가능성 낮음. v2 결과가 이 가설을 반박함.

---

## 💡 근본 원인 (최종 결론)

**가설 A (베이스 모델 불일치)가 80% 이상 확실**

### 기술적 설명

```
mahmoudmohamed 어댑터
→ 실제로는 Qwen 4B용으로 학습
→ Config만 "1.5B"로 편집 (공유 목적?)
→ 1.5B 모델에 로드 시 dimension 불일치
→ PEFT 라이브러리가 오류 없이 로드 (graceful degradation)
→ 하지만 weights가 제대로 align 안 됨
→ 모델이 degenerate state
→ 기본값 0.0만 출력
```

### 쉬운 비유

```
큰 옷(4B용 어댑터)을 작은 사람(1.5B 모델)한테 입혔더니
옷이 헐렁헐렁해서 제대로 동작 안 하는 것

정확한 사이즈(seojinpark의 1.5B-GPTQ-Int4)를 입혀야
제대로 작동함
```

---

## 📚 배운 점

### 1. Config 파일은 거짓말할 수 있다

#### ❌ 잘못된 접근
```python
# Config만 보고 믿기
config = load_config("adapter_config.json")
if config["base_model"] == "Qwen-1.5B":
    model = load_adapter()  # 위험!
```

#### ✅ 올바른 접근
```python
# 교차 검증
def validate_adapter_safety(adapter_path):
    checks = {}

    # 1. Config 확인
    config = load_config(f"{adapter_path}/adapter_config.json")
    checks["config_base_model"] = config["base_model_name_or_path"]

    # 2. 데이터셋 이름 확인
    dataset_name = get_dataset_name(adapter_path)  # "reddit-4b-think"
    checks["dataset_name_conflict"] = "4b" in dataset_name and "1.5b" in config["base_model"]

    # 3. 학습 아티팩트 확인
    checks["has_training_artifacts"] = (
        os.path.exists(f"{adapter_path}/train.pkl") and
        os.path.exists(f"{adapter_path}/val.pkl")
    )

    # 4. 소규모 테스트
    predictions = test_on_3_samples(adapter_path)
    checks["output_diversity"] = np.std(predictions) > 0.05

    # 5. 분포 정상성
    checks["distribution_normal"] = (
        0.2 < np.mean(predictions) < 0.8 and
        np.max(predictions) > 0.3
    )

    if not all(checks.values()):
        raise ValidationError(f"Failed checks: {checks}")

    return True
```

---

### 2. 100% 성공 지표가 실패를 숨길 수 있다

#### 착각
```
파싱 성공률: 100% ✅
→ "완벽하게 작동한다!"
```

#### 실제
```python
predictions = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

# 이런 체크가 필요했다
assert np.std(predictions) > 0.05, "Too uniform! All values identical."
assert np.max(predictions) > 0.3, "No high scores! Model not predicting violations."
assert 0.2 < np.mean(predictions) < 0.8, "Skewed distribution!"
```

#### 프로덕션 모니터링 확장
```python
class ModelMonitor:
    def __init__(self):
        self.metrics = {
            "parsing_success_rate": [],
            "prediction_mean": [],
            "prediction_std": [],
            "prediction_min": [],
            "prediction_max": []
        }

    def log_batch(self, predictions, parsing_success):
        self.metrics["parsing_success_rate"].append(parsing_success)
        self.metrics["prediction_mean"].append(np.mean(predictions))
        self.metrics["prediction_std"].append(np.std(predictions))
        self.metrics["prediction_min"].append(np.min(predictions))
        self.metrics["prediction_max"].append(np.max(predictions))

        # 이상 탐지
        if parsing_success == 1.0 and np.std(predictions) < 0.01:
            alert("⚠️ 100% parsing but degenerate outputs!")
```

---

### 3. 2시간 Time-boxing으로 분석 마비 방지

#### 실제 투자 시간
```
Tier 1 v1: 2시간 → 실패 → 분석 30분
Tier 1 v2: 2시간 → 실패 → 분석 30분
───────────────────────────────────────
총: 5시간
```

#### 만약 무한정 디버깅했다면?
```
❌ 20시간 낭비 가능성
❌ Tunnel vision (프롬프트만 계속 수정)
❌ 다른 접근법 (DeBERTa ensemble) 시도 못 함
```

#### Time-boxing Rule
```
[ 2시간 투자 ]
    ↓
[ 실패 ]
    ↓
[ 30분 분석 ]
    ↓
명확한 다음 방향 있음? ─ YES → 진행
                       │
                      NO → PIVOT!
                            (완전히 다른 접근)
```

**교훈:** 제한된 시간에 우선순위 결정 능력

---

## 🏭 프로덕션 환경에서 동일한 패턴

### 시나리오 1: CI/CD 파이프라인 장애

**상황:**
```
새로운 LoRA 어댑터를 프로덕션에 배포
→ 모든 예측 API가 0% fraud 반환
→ 실제로는 fraud가 있는데 탐지 못 함
```

**디버깅 과정:**
```python
# 1. 모델 로딩 오류?
→ 없음 ✅ (로그에 "Model loaded successfully")

# 2. 입력 데이터 문제?
→ 정상 ✅ (입력 validation pass)

# 3. 출력 파싱 오류?
→ 100% 성공 ✅

# 4. 예측값 분포 확인
→ 전부 0.0! 🚩🚩🚩
```

**근본 원인:**
- 잘못된 어댑터 checkpoint 배포
- Config는 맞지만 실제 weights가 다른 모델용
- **내가 Kaggle에서 겪은 것과 동일한 문제**

**해결책:**
```python
# CI/CD 파이프라인에 검증 단계 추가
def pre_deployment_validation():
    # 1. Config-Model 일치성 검증
    assert verify_config_matches_model()

    # 2. 테스트 데이터로 Sanity check
    test_predictions = model.predict(test_data)
    assert np.std(test_predictions) > 0.05, "Degenerate outputs!"

    # 3. A/B test 전에 Shadow deployment
    shadow_test(new_model, production_traffic, sample_rate=0.1)

    # 4. 분포 비교
    compare_distribution(new_model, old_model, threshold=0.1)
```

---

### 시나리오 2: 모델 서빙 환경

**상황:**
A/B 테스트를 위해 여러 버전의 LoRA 어댑터를 동적으로 로딩하는 환경

**위험:**
```python
# 버전별 어댑터
adapters = {
    "v1": "lora_adapter_v1",  # Qwen 1.5B용
    "v2": "lora_adapter_v2",  # Qwen 3B용 (실수로 등록!)
}

# 런타임에 동적 로딩
selected = adapters[traffic_split()]
model = load_adapter(selected)  # v2 로딩 시 문제 발생!
```

**해결책:**
```python
class AdapterRegistry:
    def __init__(self):
        self.adapters = {}
        self.validation_cache = {}

    def register(self, name, path, base_model_required):
        # 등록 시 검증
        if not self._validate_compatibility(path, base_model_required):
            raise IncompatibleAdapterError(
                f"{path} is not compatible with {base_model_required}"
            )

        self.adapters[name] = {
            "path": path,
            "base_model": base_model_required,
            "validated_at": datetime.now()
        }

    def _validate_compatibility(self, adapter_path, base_model):
        # 내가 Kaggle에서 배운 검증 로직 적용
        config = load_config(f"{adapter_path}/adapter_config.json")

        # Config 일치성
        if config["base_model_name_or_path"] != base_model:
            return False

        # 학습 아티팩트 존재
        if not os.path.exists(f"{adapter_path}/train.pkl"):
            return False

        # 소규모 테스트
        test_pred = quick_test(adapter_path, base_model)
        if np.std(test_pred) < 0.01:  # Degenerate!
            return False

        return True
```

---

## 🔄 다음에 다르게 할 것

### 만약 2시간이 더 있었다면

```python
# 즉시 실행할 것
model = load_qwen_1_5b_gptq()  # seojinpark 어댑터의 정확한 base model
adapter = load_adapter("seojinpark/fold3")

predictions = predict(test_samples)
# 예상: [0.23, 0.87, 0.05, 0.91, 0.34, ...]
#       ↑ 다양한 값 출력!

# 이걸로 가설 A를 100% 확인 가능
```

**예상 결과:**
- LB 점수: 0.91-0.915 (단일 fold)
- Bronze medal 권!

---

### 만약 2일이 더 있었다면

#### 1. 어댑터 호환성 자동 검증 도구 개발
```python
def create_adapter_compatibility_matrix():
    """15+ 공개 어댑터 전수 조사"""
    adapters = discover_public_adapters("jigsaw", "qwen")

    results = []
    for adapter in adapters:
        result = {
            "name": adapter.name,
            "config_base_model": adapter.config["base_model"],
            "dataset_name": adapter.dataset_name,
            "has_artifacts": check_artifacts(adapter),
            "test_output": test_on_3_samples(adapter),
            "compatible": validate(adapter)
        }
        results.append(result)

    df = pd.DataFrame(results)
    df.to_csv("adapter_compatibility_matrix.csv")
    return df[df["compatible"] == True]
```

#### 2. Multi-fold Ensemble
```python
# seojinpark fold0~4 전부 사용
folds = ["fold0", "fold1", "fold2", "fold3", "fold4"]
predictions = []

for fold in folds:
    adapter = load_adapter(f"seojinpark/{fold}")
    pred = model_with_adapter.predict(test)
    predictions.append(pred)

# Rank averaging (calibration 불필요)
ensemble_pred = rank_average(predictions)

# 예상 LB: 0.92-0.925 (Bronze medal!)
```

---

## 📝 결론

이 실패 경험은 단순한 "대회 실패"가 아니라, **프로덕션 ML 시스템에서 반드시 겪게 될 문제를 미리 경험**한 것입니다.

### 핵심 교훈 (Take-aways)

1. **외부 리소스는 반드시 검증하라**
   - Config, 데이터셋 이름, 학습 아티팩트, 소규모 테스트

2. **단일 지표에 속지 마라**
   - Parsing 100% ≠ 올바른 동작
   - 분포 모니터링 필수

3. **Time-boxing으로 분석 마비 방지**
   - 2시간 rule
   - 명확한 pivot 기준

4. **프로덕션 환경 연결**
   - 이 디버깅 패턴은 CI/CD, 모델 서빙에 직접 적용 가능

---

**이 분석이 도움이 되셨다면 ⭐ GitHub Star를 눌러주세요!**

[← 메인 README로 돌아가기](README_KR.md)

---

**마지막 업데이트:** 2024년 10월 25일
